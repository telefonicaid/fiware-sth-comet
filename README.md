#<a id="section0"></a> Short Time Historic (aka. Comet)

[![Join the chat at https://gitter.im/telefonicaid/fiware-sth-comet](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/telefonicaid/fiware-sth-comet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

* [Introduction] (#section1)
    * [Why Comet?] (#section1.1)
    * [Consuming raw data] (#section1.2)
    * [Consuming aggregated time series information] (#section1.3)
    * [Updating aggregated time series information] (#section1.4)
    * [Removing raw and aggregated time series information] (#section1.5)
    * [Data migration between data models] (#section1.6)
* [Dependencies](#section2)
* [Installation](#section3)
    * [Cloning the Github repository](#section3.1)
    * [Using a RPM package](#section3.2)
* [Automatic deployment using Docker](#section4)
* [Running the STH server](#section5)
* [STH component test coverage](#section7)
* [Performance tests](#section8)
* [Additional resources] (#section9)
* [Contribution guidelines] (#section10)
* [Contact](#section11)

##<a id="section1"></a> Introduction
The Short Time Historic (STH, aka. Comet) is a component of the [FIWARE](https://www.fiware.org/) ecosystem
in charge of providing aggregated time series information about the evolution in
time of entity attribute values registered using the
<a href="http://catalogue.fiware.org/enablers/publishsubscribe-context-broker-orion-context-broker" target="_blank">Orion Context Broker</a>,
an implementation of the publish/subscribe context management system exposing NGSI9 and
<a href="http://technical.openmobilealliance.org/Technical/technical-information/release-program/current-releases/ngsi-v1-0">NGSI10</a> interfaces.

The aggregated time series information is stored in a MongoDB instance. This information can be generated by 2 main
means:

1. The STH component can directly subscribe to the Context Broker to receive notifications when the entity attribute
values change, calculating the aggregated time series information and storing it in the MongoDB instance.
This option is called the minimalist option.
2. A new sink will be enabled in the <a href="https://github.com/telefonicaid/fiware-cygnus" target="_blank">Cygnus</a>
component to calculate and to update the aggregated time series information
as the entity attribute values change over time. Using Cygnus adds a set of capabilities not available in the minimalist
option such as advanced filtering regarding the attributes to consider in the time series, advanced flow and congestion
management, amongst others. This option is provided as a
<a href="https://github.com/telefonicaid/fiware-cygnus/blob/master/flume/doc/devel/add_new_sink.md" target="_blank">Cygnus sink</a>
just like the <a href="https://github.com/telefonicaid/fiware-cygnus/tree/master/flume/src/main/java/es/tid/fiware/fiwareconnectors/cygnus/sinks" target="_blank">other data stores</a> already supported by Cygnus.
This option is the formal one.

Since both mechanisms (the formal one using Cygnus and the minimalist one using the STH component directly) update the same database,
it is the responsibility of the people or software in charge of creating the needed subscriptions to avoid updating the
time series database twice (i.e. to avoid enabling both mechanisms at the same time). This would happen if both mechanisms
are enabled for the same attribute of the same entity.

Regarding the aggregated time series information provided by the STH component, there are 3 main concepts which are
important to know about:

* <b>Resolution</b> or <b>aggregation period</b>: The time period by which the aggregated time series information is grouped.
Possible valid resolution values are: month, day, hour, minute and second.
* <b>Origin</b>: For certain resolution, it is the origin of time for which the aggregated time series
information applies. For example, for a resolution of minutes, a valid origin value could be: ```2015-03-01T13:00:00.000Z```,
meaning the 13th hour of March, the 3rd, 2015. The origin is stored using UTC time to avoid locale issues.
* <b>Offset</b>: For certain resolution, it is the offset from the origin for which the aggregated time series
information applies. For example, for a resolution of minutes and an origin ```2015-03-01T13:00:00.000Z```, an offset of 10
refers to the 10th minute of the concrete hour pointed by the origin. In this example, there would be a maximum of 60
offsets from 0 to 59 corresponding to each one of the 60 minutes within the concrete hour.
* <b>Samples</b>: For a triple resolution, origin and offset, it is the number of samples, values, events or notifications available
for that concrete offset from the origin.

###<a id="section1.1"></a> Why Comet?

Since most of the components which conform the [FIWARE](https://www.fiware.org/) ecosystem have astrological
names, we decided to follow that path in the case of the STH too. Since the STH is in charge of collecting historical information
about the values attributes took over time, we decided to name it "Comet", in reference to the tails comets leave on their way as they move.

###<a id="section1.2"></a> Consuming raw data

The STH component exposes an HTTP REST API to let external clients query the raw events (aka. raw data) from which the
aggregated time series information is generated. A typical URL querying for this information using a GET request is the following:

<pre>http://localhost:8666/STH/v1/contextEntities/type/&lt;entityType&gt;/id/&lt;entityId&gt;/attributes/&lt;attrName&gt;?hLimit=3&hOffset=0&dateFrom=2014-02-14T00:00:00.000Z&dateTo=2014-02-14T23:59:59.999Z</pre>

The entries between "<" and ">" in the URL path depend on the concrete case (type of data, entity and attribute) being queried.

The requests can make use the following query parameters:

* <b>lastN</b>: Only the requested last entries should be returned. It is a mandatory parameter if no hLimit and hOffset are provided.
* <b>hLimit</b>: In case of pagination, the number of entries per page. It is a mandatory parameter if no lastN is provided.
* <b>hOffset</b>: In case of pagination, the offset to apply to the requested search of raw data. It is a mandatory parameter if no lastN is provided.
* <b>dateFrom</b>: The origin of time from which the raw data is desired. It is an optional parameter.
* <b>dateTo</b>: The end of time until which the raw data is desired. It is an optional parameter.
* <b>filetype</b>: The raw data can be requested as a file setting this query parameter to the desired file type.
Currently, the only supported value and file type is "csv".  It is an optional parameter.

An example response provided by the STH component to a request such as the previous one could be the following:

<pre>
{
    "contextResponses": [
        {
            "contextElement": {
                "attributes": [
                    {
                        "name": "attrName",
                        "values": [
                            {
                                "recvTime": "2014-02-14T13:43:33.306Z",
                                "attrValue": "21.28"
                            },
                            {
                                "recvTime": "2014-02-14T13:43:34.636Z",
                                "attrValue": "23.42"
                            },
                            {
                                "recvTime": "2014-02-14T13:43:35.424Z",
                                "attrValue": "22.12"
                            }
                        ]
                    }
                ],
                "id": "entityId",
                "isPattern": false
            },
            "statusCode": {
                "code": "200",
                "reasonPhrase": "OK"
            }
        }
    ]
}
</pre>

Notice that a paginated response has been requested with a limit of 3 entries and an offset of 0 entries (first page).

It is important to note that if a valid query is made but it returns no data (for example because there is no raw data
for the specified time frame), a response with code `200` is returned including an empty `values` property array, since it is a valid
query.

[Top](#section0)

###<a id="section1.3"></a> Consuming aggregated time series information

The STH component exposes an HTTP REST API to let external clients query this aggregated time series information. A
typical URL querying for this information using a GET request is the following:

<pre>http://localhost:8666/STH/v1/contextEntities/type/&lt;entityType&gt;/id/&lt;entityId&gt;/attributes/&lt;attrName&gt;?aggrMethod=sum&aggrPeriod=second&dateFrom=2015-02-22T00:00:00.000Z&dateTo=2015-02-22T23:00:00.000Z</pre>

The entries between "<" and ">" in the URL path depend on the concrete case (type of data, entity and attribute) being queried.

The requests can make use the following query parameters:

* <b>aggrMethod</b>: The aggregation method. The STH component supports the following aggregation methods: `max` (maximum
value), `min (minimum value), `sum` (sum of all the samples) and `sum2` (sum of the square value of all the samples) for numeric
attribute values and `occur` for attributes values of type string. Combining
the information provided by these aggregated methods with the number of samples, it is possible to calculate probabilistic
values such as the average value, the variance as well as the standard deviation. It is a mandatory parameter.
* <b>aggrPeriod</b>: Aggregation period or resolution. A fixed resolution determines the origin time format and the
possible offsets. It is a mandatory parameter.
* <b>dateFrom</b>: The origin of time from which the aggregated time series information is desired. It is an optional parameter.
* <b>dateTo</b>: The end of time until which the aggregated time series information is desired. It is an optional parameter.

An example response provided by the STH component to a request such as the previous one (for a numeric attribute value) could be the following:

<pre>
{
    "contextResponses": [
        {
            "contextElement": {
                "attributes": [
                    {
                        "name": "attrName",
                        "values": [
                            {
                                "_id": {
                                    "origin": "2015-02-18T02:46:00.000Z",
                                    "resolution": "second"
                                },
                                "points": [
                                    {
                                        "offset": 13,
                                        "samples": 1,
                                        "sum": 34.59
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "id": "entityId",
                "isPattern": false
            },
            "statusCode": {
                "code": "200",
                "reasonPhrase": "OK"
            }
        }
    ]
}
</pre>

In this example response, aggregated time series information for a resolution of seconds is returned.
This information has as its origin the 46nd minute, of the 2nd hour of February, the 18th, 2015. And includes data for the
13th second, for which there is a sample and the sum (and value of that sample) is 34.59.

On the other hand, if the attribute value was of type string, a query such as the following (with `aggrMethod` as `occur`)
sent to the STH component:

<pre>http://localhost:8666/STH/v1/contextEntities/type/&lt;entityType&gt;/id/&lt;entityId&gt;/attributes/&lt;attrName&gt;?aggrMethod=occur&aggrPeriod=second&dateFrom=2015-02-22T00:00:00.000Z&dateTo=2015-02-22T23:00:00.000Z</pre>

may end up receiving the following payload as a possible response:

<pre>
{
    "contextResponses": [
        {
            "contextElement": {
                "attributes": [
                    {
                        "name": "attrName",
                        "values": [
                            {
                                "_id": {
                                    "origin": "2015-02-18T02:46:00.000Z",
                                    "resolution": "second"
                                },
                                "points": [
                                    {
                                        "offset": 35,
                                        "samples": 34,
                                        "occur": {
                                            "string01": 7,
                                            "string02": 4,
                                            "string03": 5,
                                            "string04": 6,
                                            "string05": 12
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "id": "entityId",
                "isPattern": false
            },
            "statusCode": {
                "code": "200",
                "reasonPhrase": "OK"
            }
        }
    ]
}
</pre>

It is important to note that if a valid query is made but it returns no data (for example because there is no aggregated data
for the specified time frame), a response with code `200` is returned including an empty `values` property array, since it is a valid
query.

Another very important aspect is that since the strings are used as properties in the generated aggregated data, the [limitations
to this regard imposed by MongoDB](https://docs.mongodb.org/manual/faq/developers/#dollar-sign-operator-escaping) must be followed. More concretely: "In some cases, you may wish to build a BSON object with a user-provided key.
In these situations, keys will need to substitute the reserved $ and . characters. Any character is sufficient, but consider
using the Unicode full width equivalents: U+FF04 (i.e. “＄”) and U+FF0E (i.e. “．”).". Consequently, take into consideration
that if the textual values stored in the attributes for which aggregated data is being generated contain the `$` or the `.` characters,
they will be substituted for their Javascript Unicode full width equivalents, this is: `\uFF04` instead of `$` and `\uFF0E`
instead of `.`.

[Top](#section0)

###<a id="section1.4"></a> Updating aggregated time series information

As already mentioned, there are 2 main ways to update the aggregated time series information associated to attributes.
The so-called minimalist option and the formal one.

Regarding the formal option (based on using the Cygnus component for the updating), please refer to the documentation available at the
<a href="https://github.com/telefonicaid/fiware-cygnus" target="_blank">Cygnus component repository</a>, and more concretely at the following links:

* <a href="https://github.com/telefonicaid/fiware-cygnus/blob/master/doc/user_and_programmer_guide/connecting_orion.md" href="_blank">Connecting Orion Context Broker and Cygnus</a>
* <a href="https://github.com/telefonicaid/fiware-cygnus/blob/master/doc/flume_extensions_catalogue/orion_mongo_sink.md" href="_blank">OrionMongoSink</a>
* <a href="https://github.com/telefonicaid/fiware-cygnus/blob/master/doc/flume_extensions_catalogue/orion_sth_sink.md" href="_blank">OrionSTHSink</a>

The other option to update the aggregated time series information consists on directly subscribing the STH component
to the Orion Context Broker to receive the corresponding notifications and generate and update the aggregated data.

In the minimalist option, the STH component calculates aggregated data grouped at certain resolutions whenever it receives
a notification from the Orion Context Broker. To this regard and as a way to subscribe the STH component to the Orion Context Broker
so it receives the attribute values of interest, the following curl command can be used:

<pre>
curl orion.contextBroker.host:1026/v1/subscribeContext -s -S --header 'Content-Type: application/json' --header 'Accept: application/json' --header 'Fiware-Service: theService' --header 'Fiware-ServicePath: theServicePath' -d @- &lt;&lt;EOF
{
    "entities": [
        {
            "type": "Room",
            "isPattern": "false",
            "id": "Room1-gtv"
        }
    ],
    "attributes": [
        "temperature"
    ],
    "reference": "http://&lt;sth.host&gt;:&lt;sth.port&gt;/notify",
    "duration": "P1M",
    "notifyConditions": [
        {
            "type": "ONCHANGE",
            "condValues": [
                "temperature"
            ]
        }
    ],
    "throttling": "PT1S"
}
EOF
</pre>

In this request, a subscription to be notified the value of the temperature attribute of the Room1 entity whenever it changes
is made to an instance of the Orion Context Broker listening at `orion.contextBroker.host:1026`.

More concretely, the `condValues` property includes a list of attributes of the entity of interest which should be tracked for changes.
If any of them changes, a new notification will be sent to the endpoint set in the `reference` property including the current values
of the attributes specified in the `attributes` property of the subscription request payload. The `condValues` and `attributes` properties
can include any attributes of the entity of interest (not necessarily the same in both lists) and their change and latest values will be
notified accordingly.

If the list of `attributes` is empty, this is interpreted by the Orion Context Broker as "notify the values of all the attributes of the selected entities".

It is important to note that the subscription expire and must be re-enabled. More concretely, the `duration` property sets the duration of the subscription.
One month in the proposed example.

On the other hand, for the time being the STH component only is able to manage notifications in JSON format and consequently
it is very important to set the `Accept` header to `application/json`.

Last but not least, the `throttling` makes it possible to control the frequency of the notifications. In this sense and for
this concrete example, the Orion Context Broker will send notifications separated 1 second in time the least. This is, the
time between notifications will be at least 1 second. Depending on the resolution of the aggregated data you are interested
in, the `throttling` should be fine-tuned accordingly.

[Top](#section0)

###<a id="section1.5"></a> Removing raw and aggregated time series information

Last, but not least, the STH component exposes through its REST API the possibility to remove previously registered
raw and aggregated data. Due to the sensible nature of these operations, they should be used with caution since their
effects cannot be undone.

The STH component exposes 3 main URLs for data removal, all of them invoked using `DELETE`
as the HTTP method and including the service and service path information as headers (`Fiware-Service` and
`Fiware-ServicePath` headers, respectively) such as in the `curl` examples included in the previous sections. The
provided URLs are the following ones:

1. Removing all the data associated to certain service and service path:
<pre>http://localhost:8666/STH/v1/contextEntities</pre>
2. Removing all the data associated to certain entity, service and service path:
<pre>http://localhost:8666/STH/v1/contextEntities/type/{entityType}/id/{entityId}</pre>
3. Removing all the data associated to certain attribute of certain entity, service and service path:
<pre>http://localhost:8666/STH/v1/contextEntities/type/{entityType}/id/{entityId}/attributes/{attrName}</pre>

It is important to note that the data removal accomplished depends on the value of the `SHOULD_STORE` configuration
parameter. This means that depending on its value, only the associated data (raw, aggregated or both) will be removed.

[Top](#section0)

###<a id="section1.6"></a> Data migration between data models

The STH component supports 3 alternative data models when storing the raw and aggregated data into the database:

1. One collection per service path.
2. One collection per entity.
3. One collection per attribute.

As their names reflect, each one of the supported data models stores the raw and aggregated data into one collection per service path, entity or attribute respectively.

The default data model is the collection per entity one, but sometimes to get the best performance out of the MongoDB
database where the raw and aggregated data is stored, alternative data models are needed. This is the reason why we
introduced the data model migration tools.

To set the desired data model to be used, please take a look at [Running the STH server](#section5) below.

To run the data migration tool, please execute the following command:
```
./bin/sth_database_model
```
which will present the command help information:
```
Usage: sth_database_model [options]

  Options:

    -h, --help                         output usage information
    -V, --version                      output the version number
    -a, --analysis                     prints the results of the data model analysis including the databases and collections which need to be migrated to the currently configured data model (mandatory if not -m or --migrate)
    -m, --migrate                      migrates to the currently configured data model all the databases and collections which has been created using a distinct data model (mandatory if not -a or --analysis)
    -v, --verbose [documents]          shows migration progress information if the number of documents to migrate in the collection is bigger or equal to the optional value passed (1 if no value passed)
    -r, --remove-collection            the original data model collection will be removed to avoid conflict if migrating back to that data model in the future
    -u, --update-collection            the migration will take place even if the target collections already exist combining the data of the original and target collections (use this option with special care since the migration operation is not idempotent for the aggregated data collections)
    -f, --full                         the migration will continue with the pending collections in case a previous collection throws an error and cannot be migrated (it is recommended to be used with the -r option to avoid subsequent  migrations of the same aggregated data collections)
    -d, --database <databaseName>      only this database will be taken into consideration for the analysis and/or migration process
    -c, --collection <collectionName>  only this collection will be taken info consideration, a database is mandatory if a collection is set
    -x, --dictionary <dictionary>      the path to a file including a dictionary to resolve the names of the collections to be migrated to their associated data (i.e., service path, entity id, entity type, attribute name and attribute type) (it is expected as a CSV file with lines including the following info: <collection-name>,<service-path>,<entity-id>,<entity-type>,<attribute-name>,<attribute-type>, some of which may not apply and can be left as blank)
```

Special care should be taken when requesting a data model migration since the migration of aggregated data is not an idempotent operation if the target data model collections already exist. In this case, the already existent data stored in these collections is combined with the one stored in the original data model collections pending migration. Based on this fact, we suggest the next procedure when considering a data model migration:

1. Check the current configured data model used by the STH component based on the options detailed in [Running the STH server](#section5) below. Typically it will be the default collection per entity data model.

2. Get a data model analysis report about the databases and collections which need to be migrated to the new desired data model (set in the `DATA_MODEL` environment variable) running the following command:
<pre>LOGOPS_FORMAT=dev DATA_MODEL=collection-per-service-path ./bin/sth_database_model -a</pre>

3. Request the desired data model migration (set in the `DATA_MODEL` environment variable) without forcing the update of the target data model collections running the following command:
<pre>LOGOPS_FORMAT=dev DATA_MODEL=collection-per-service-path ./bin/sth_database_model -a -m</pre>
If any of the target collections already exist, the data model migration will stop and it will not be made for the first already existent target data model collection, either for any subsequent collection. If none of the target collections already exist, the data model migration will successfully complete.

  3.1. If the data model migration completed successfully, remove the original data model collections already migrated to avoid problems if in the future you decide to go back to the original data model. Information about the successfully migrated collections is provided in the logs. Setting the `-r` option when running the command mentioned in point 3 will make this removal automatically for you. The data model migration has successfully finished.

  3.2. If the data model migration did not complete successfully because any of the target data model collections already exist:

    3.2.1. Remove the original data model collections which were successfully migrated, if any, so they are not migrated again in the future (details about the successfully migrated collections is provided in the logs). The `-r` option will make this removal automatically for you when running the command mentioned in point 3.

    3.2.2. You have to decide if the target data model collection causing the conflict contains valuable data. If they does, just keep it. If it does not, just remove it.

    3.2.3. If you decided to keep the target data model collection causing the conflict since it contains valuable data, force its data model migration using the following command:
  <pre>LOGOPS_FORMAT=dev DATA_MODEL=collection-per-service-path ./bin/sth_database_model -a -m -d &lt;database_name&gt; -c &lt;original_data_model_collection_to_be_migrated&gt; -u</pre> The original data model collection will be combined with the already existent data stored in the target data model collection.

  3.2.4. Remove the `<original_data_model_collection_to_be_migrated>` collection whose migration you just forced so it is not migrated again in the future.

4. Get back and repeat from point 3.

Currently the only data model migration supported is the default collection per entity data model to the collection per service path data model.

[Top](#section0)

##<a id="section2"></a> Dependencies
The STH component is a Node.js application which depends on certain Node.js modules as stated in the ```project.json``` file.

Apart from these Node.js modules, the STH component also needs a running MongoDB instance where the aggregated time series
information is stored for its proper functioning. Since the STH component uses MongoDB update operators (see
<a href="http://docs.mongodb.org/v2.6/reference/operator/update/" target="_blank">http://docs.mongodb.org/v2.6/reference/operator/update/</a>)
such as the ```$max``` and the ```$min``` update operators which were introduced in version 2.6, there is a dependency
of the STH component with this concrete version of the MongoDB instance where the aggregated data will be stored.
Consequently, a MongoDB instance version &gt;= 2.6 is needed to store the aggregated time series information.

[Top](#section0)

##<a id="section3"></a> Installation

### <a id="section3.1"></a> Cloning the GIthub repository

1. Clone the repository:
<pre> git clone https://github.com/telefonicaid/fiware-sth-comet.git </pre>
2. Get into the directory where the STH repository has been cloned:
<pre> cd fiware-sth-comet/ </pre>
3. Install the Node.js modules and dependencies:
<pre> npm install </pre>
The STH component server is ready to be started as a Node application.

[Top](#section0)

### <a id="section3.2"></a> Using a RPM package

#### Package generation

**Prerequisites:** To generate the RPM package from the STH component sources
it is needed to have the rpm build tools (rpmbuild executable), Node and the
npm utilities, as well as an Internet connection to download the required Node modules.

To generate the RPM package for the STH component, execute the following
command from the root of the STH component:

`./rpm/create-rpm.sh -v <version> -r <release>`

If everything goes fine, a new RPM package such as `./rpm/RPMS/x86_64/fiware-sth-comet-<version>-<release>.x86_64.rpm`
will be created.

Execute `./rpm/create-rpm.sh -h` for more information about the RPM package creation script.

#### Installation, upgrade and removal

**Prerequisites:** Node is needed to install the generated STH component RPM package.

To install or upgrade the STH component, execute: `sudo rpm -Uvh fiware-sth-comet-<version>-<release>.x86_64.rpm`

After the installation, the following files and directories are created:
```
/etc/init.d
└── sth

/etc/logrotate.d
└── logrotate-sth-daily

/var/log/sth

/var/run/sth

/opt/sth
├── conf
│   └── <empty> Here is where instances are configured
├── node_modules
│   └── <node modules directory structure and files>
├── package.json
└── src
    └── <STH SW files>
```

To remove a previous STH component installation, execute: `sudo rpm -e fiware-sth-comet`

#### Configuration

STH is able to start multiple instances using the [sth](rpm/SOURCES/etc/init.d/sth "sth") service script
by adding and configuring certain files as detailed next.

To start multiple instances, one configuration file per instance has to be included in
the `/opt/sth/conf` directory. It is important to note that the default installation includes
preconfigured instances.

It is important to change the `STH_PORT` value included in the configuration files
to a value not used by other STH instances/services. It is also a good practice to change
the `LOG_FILE_NAME` value to avoid getting the logs from several instances mixed.

The [init.d](rpm/SOURCES/etc/init.d/sth "sth") service script includes the following operations:

* **start** (`sudo /sbin/service sth start [<instance>]`): if `<instance>` is not provided, the script starts
an instance per configuration file found in the `/opt/sth/conf` directory matching the `sth_*.conf` template.
If `<instance>` is provided, a configuration file named `sth_<instance>.conf` is searched in the `/opt/sth/conf` directory
and the corresponding instance is started.
* **stop** (`sudo /sbin/service sth stop [<instance>]`): if `<instance>` is not provided, the script stops
all the instances by listing all pid files under `/var/run/sth` matching the pattern `sth_*.pid`.
If `<instance>` is provided, the scripts stops the instance with the associated pid file `/var/run/sth/sth_<instance>.pid`
* **status** (`sudo /sbin/service sth status [<instance>]`): The status operation shows information about one or more running instances
following the same procedure detailed in the `stop` operation.
* **restart** (`sudo /sbin/service sth stop [<instance>]`): The restart operation executes a `stop` operation followed by a `start` operation
according to the procedure detailed in those operations.

An example [`sth_default.conf`](rpm/EXAMPLES/sth_default.conf) file has been included in this Github repository to guide the STH instance
configuration.

Last but not least, the STH process (a `node` process) runs the as `sth` user.

[Top](#section0)

##<a id="section4"></a> Automatic deployment using Docker
To ease the testing and deployment of the STH component we have prepared a Docker repository which can be found at
[https://registry.hub.docker.com/u/fiwareiotplatform/iot-sth/](https://registry.hub.docker.com/u/fiwareiotplatform/iot-sth/),
including all the information needed to try and to deploy the STH component via the execution of a simple Docker command.

On the other hand a [`Dockerfile`](https://github.com/telefonicaid/fiware-sth-comet/blob/master/Dockerfile) and a
[`docker-compose.yml`](https://github.com/telefonicaid/fiware-sth-comet/blob/master/docker-compose.yml) files have also been
included in this very repository to quickly and easily start your own instance of the STH component, even including the
associated MongoDB instance where all the data will be stored.

To do it, follow the next steps once you have installed Docker in your machine:

1. Navigate to the path where this repository was cloned.

2. Compose and run the new STH component image:
```bash
docker-compose up
```

[Top](#section0)

##<a id="section5"></a>Running the STH server
1. To run the STH server, just execute from the STH directory:
```bash
./bin/sth
```

The STH component provides the user with 2 mechanisms to configure the component to the concrete needs of the user:

* Environment variables, which can be set assigning values to them or using the `sth_default.conf` file if a packaged
version of the STH component is used.
* The `config.js` file located at the root of the STH component code, a JSON formatted file including the configuration properties.

It is important to note that environment variables, if set, take precedence over the properties defined in the
`config.js` file.

On the other hand, it is also important to note that the aggregation resolutions can only be configured using the
`config.js` file and
consequently this is the preferred way to configure the STH component behavior. The mentioned resolutions can be configured using
the `config.server.aggregation` property in the `config.js` file
including the desired resolution to be used when aggregating data. Accepted resolution values include: `month`, `day`, `hour`, `minute` and `second`.

In case of preferring using environment variables, the script accepts the following parameters as environment variables:

- STH_HOST: The host where the STH server will be started. Optional. Default value: "localhost".
- STH_PORT: The port where the STH server will be listening. Optional. Default value: "8666".
- FILTER_OUT_EMPTY: A flag indicating if the empty results should be removed from the response. Optional. Default value: "true".
- TEMPORAL_DIR: A relative path from the STH home directory to a directory where the temporary files generated by the STH component are stored.
These files are generated before returning them when the `filetype` is included in any data retrieval request.
Default value: "temp".
- DEFAULT_SERVICE: The service to be used if not sent in the Orion Context Broker notifications. Optional. Default value: "testservice".
- DEFAULT_SERVICE_PATH: The service path to be used if not sent in the Orion Context Broker notifications. Optional. Default value: "/testservicepath".
- DATA_MODEL: The STH component supports 3 alternative data models when storing the raw and aggregated data into the database:
1) one collection per attribute, 2) one collection per entity and 3) one collection per service path. The possible values are: "collection-per-attribute", "collection-per-entity" and "collection-per-service-path" respectively. Default value: "collection-per-entity".
- DB_USERNAME: The username to use for the database connection. Optional. Default value: "".
- DB_PASSWORD: The password to use for the database connection. Optional. Default value: "".
- DB_URI: The URI to use for the database connection. This does not include the 'mongo://' protocol part (see a couple of examples below).
Optional. Default value: "localhost:27017".
- REPLICA_SET: The name of the replica set to connect to, if any. Default value: "".
- DB_PREFIX: The prefix to be added to the service for the creation of the databases. More information below. Optional. Default value: "sth_".
- COLLECTION_PREFIX: The prefix to be added to the collections in the databases. More information below. Optional. Default value: "sth_".
- POOL_SIZE: The default MongoDB pool size of database connections. Optional. Default value: "5".
- WRITE_CONCERN: The <a href="http://docs.mongodb.org/manual/core/write-concern/" target="_blank">write concern policy</a> to apply when writing data to the MongoDB database. Default value: "1".
- SHOULD_STORE: Flag indicating if the raw and/or aggregated data should be persisted. Valid values are: "only-raw", "only-aggregated" and "both". Default value: "both".
- SHOULD_HASH: Flag indicating if the raw and/or aggregated data collection names should include a hash portion. This is mostly
due to MongoDB's limitation regarding the number of bytes a namespace may have (currently limited to 120 bytes). In case of hashing,
information about the final collection name and its correspondence to each concrete service path, entity and (if applicable) attribute
is stored in a collection named `COLLECTION_PREFIX + "collection_names"`. Default value: "false".
- TRUNCATION_EXPIRE_AFTER_SECONDS: Data from the raw and aggregated data collections will be removed if older than the value specified in seconds.
In case of raw data the reference time is the one stored in the `recvTime` property whereas in the case of the aggregated data
the reference of time is the one stored in the `_id.origin` property. Set the value to 0 not to apply this time-based truncation
policy. Default value: "0".
- TRUNCATION_SIZE: The oldest raw data (according to insertion time) will be removed if the size of the raw data collection
gets bigger than the value specified in bytes. Set the value to 0 not to apply this truncation
policy. Take into consideration than the "size" configuration parameter is mandatory in case size collection truncation
is desired as required by MongoDB. Default value: "0". Notice that this configuration parameter does not affect the aggregated
data collections since MongoDB does not currently support updating documents in capped collections which increase the size of the documents.
Notice also that in case of the raw data, the size-based truncation policy takes precedence over the TTL one. More concretely,
if "size" is set, the value of "exporeAfterSeconds" is ignored for the raw data collections since currently MongoDB does not support TTL in capped collections.
Default value: "0".
- TRUNCATION_MAX: The oldest raw data (according to insertion time) will be removed if the number of documents in the raw data
collections goes beyond the specified value. Set the value to 0 not to apply this truncation policy.
Notice that this configuration parameter does not affect the aggregated data collections since MongoDB does not
currently support updating documents in capped collections which increase the size of the documents.
Default value: "0".
- IGNORE_BLANK_SPACES: Attribute values to one or more blank spaces should be ignored and not processed either as
raw data or for the aggregated computations. Default value: "true".
- LOGOPS_LEVEL: The log level to use. Possible values are: "DEBUG", "INFO", "WARN", "ERROR" and "FATAL". Since the STH component uses the logops package for logging,
for further information check out the [logops](https://www.npmjs.com/package/logops) npm package information online. Default value: "INFO".
- LOGOPS_FORMAT: The log format to use. Possible values are: "json" (writes logs as JSON), "dev" (for development, used when the NODE_ENV variable is set to
'development'). Since the STH component uses the logops package for logging, for further information please check out the
[logops](https://www.npmjs.com/package/logops) npm package information online. Default value: "json".
- PROOF_OF_LIFE_INTERVAL: The time in seconds between proof of life logging messages informing that the server is up and running normally. Default value: "60".

For example, to start the STH server listening on port 7777, connecting to a MongoDB instance listening on mymongo.com:27777 and
without filtering out the empty results, use:

```bash
STH_PORT=7777 DB_URI=mymongo.com:27777 FILTER_OUT_EMPTY=false ./bin/sth
```

On the other hand, in case of connecting to a MongoDB replica set composed of 3 machines with IPs addresses 1.1.1.1, 1.1.1.2, 1.1.1.3
listening on ports 27771, 27772 and 27773, respectively, use:

```bash
DB_URI=1.1.1.1:27771,1.1.1.2:27772,1.1.1.3:27773 ./bin/sth
```

The STH component creates a new database for each <a href="https://forge.fiware.org/plugins/mediawiki/wiki/fiware/index.php/Publish/Subscribe_Broker_-_Orion_Context_Broker_-_User_and_Programmers_Guide#Multi_service_tenancy" target="_blank">service</a>.
The name of these databases will be the concatenation of the DB_PREFIX environment variable and the service, using an underscore ("_") as the separator.

As already mentioned, all this configuration parameters can also be adjusted using the
`config.js` file whose contents are self-explanatory.

It is important to note that there is a limitation of 120 bytes for the namespaces (concatenation of the database name and
collection names) in MongoDB (see <a href="http://docs.mongodb.org/manual/reference/limits/#namespaces" target="_blank">http://docs.mongodb.org/manual/reference/limits/#namespaces</a>
for further information). Related to this, the STH generates the collection names using 2 possible mechanisms:

1. <u>Plain text</u>: In case the `SHOULD_HASH` configuration parameter is set to 'false' (the default option), the collection names are
generated as a concatenation of the `COLLECTION_PREFIX` plus the service path plus the entity id plus the entity type
plus '.aggr' for the collections storing the aggregated data. The length of the collection name plus the `DB_PREFIX` plus
the database name (or service) should not be more than 120 bytes using UTF-8 format or MongoDB will complain and will not
create the collection, and consequently no data would be stored by the STH. A warning message is logged in case this happens.

2. <u>Hash based</u>: In case the `SHOULD_HASH` option is set to something distinct from 'false', the
collection names are generated as a concatenation of the `COLLECTION_PREFIX` plus a generated hash plus '.aggr' for the
collections of the aggregated data. To avoid collisions in the generation of these hashes, they are forced to be 20 bytes
long at least. Once again, the length of the collection name plus the `DB_PREFIX` plus the database name (or service) should not
be more than 120 bytes using UTF-8 or MongoDB will complain and will not create the collection, and consequently no data
would be stored by the STH. The hash function used is SHA-512. A warning message is logged in case this happens.

In case of using hashes as part of the collection names and to let the user or developer easily recover this information,
a collection named ```DB_COLLECTION_PREFIX + _collection_names``` is created and fed with information regarding the mapping
of the collection names and the combination of concrete services, service paths, entities and attributes.

[Top](#section0)

##<a id="section7"></a> STH component test coverage
The STH component source code includes a set of tests to validate the correct functioning of the whole set of capabilities
exposed by the component. This set includes:

- Tests to check the connection to the database
- Tests to check the correct starting of the STH component
- Tests to check the STH component correctly deals with all the possible requests it may receive (including invalid URL paths (routes)
as well as all the combinations of possible query parameters)
- Tests to check the correct aggregate time series information querying after inserting random events (attribute values)
into the database
- Tests to check the correct aggregate time series information generation when receiving (simulated) notifications by a
(fake) Orion Content Broker

### Preconditions
A running instance of a MongoDB database.

### Running the tests
In order to execute the test suite you must have the Grunt client installed. You can install it using the following command
(you will need root permissions):
```bash
npm install -g grunt-cli
```

Once the client is installed and the dependencies are downloaded, you can execute the tests using:
```
grunt test
```

This will execute the functional tests and the syntax checking as well.

The test suite accepts the following parameters as environment variables which can be used to personalise them:

- SAMPLES: The number of random events which will be generated and inserted into the database. Optional. Default value: "5".
- EVENT_NOTIFICATION_CONTEXT_ELEMENTS: The number of context elements included in the simulated notifications sent to
the STH component. Optional. Default value: 3.
- ENTITY_ID: The id of the entity for which the random event will be generated. Optional. Default value: "entityId".
- ENTITY_TYPE: The type of the entity for which the random event will be generated. Optional. Default value: "entityType".
- ATTRIBUTE_NAME: The id of the attribute for which the random event will be generated. Optional. Default value: "attrName"
- ATTRIBUTE_TYPE: The type of the attribute for which the random event will be generated. Optional. Default value: "attrType".
- START_DATE: The date from which the random events will be generated. Optional. Default value: the beginning of the previous
year to avoid collisions with the testing of the Orion Context Broker notifications which use the current time.
For example if in 2015, the start date is set to "2015-01-01T00:00:00", UTC time. Be very careful if setting the start date,
since these collisions may arise.
- END_DATE: The date before which the random events will be generated. Optional. Default value: the end of the previous
year to avoid collisions with the testing of the Orion Context Broker notifications which use the current time.
For example if in 2015, the end date is set to "2014-12-31T23:59:59", UTC time. Be very careful if setting the start date,
since these collisions may arise.
- MIN_VALUE: The minimum value associated to the random events. Optional. Default value: "0".
- MAX_VALUE: The maximum value associated to the random events. Optional. Default value: "100".
- CLEAN: A flag indicating if the generated collections should be removed after the tests. Optional. Default value: "true".

For example, to run the tests using 100 samples, certain start and end data without cleaning up the database after running
the tests, use:
```bash
SAMPLES=100 START_DATE=2014-02-14T00:00:00 END_DATE=2014-02-14T23:59:59 CLEAN=false grunt test
```

In case of executing the tests with the CLEAN option set to false, the contents of the database can be inspected using the MongoDB
(```mongo```) shell.

[Top](#section0)

##<a id="section8"></a>Performance tests

The [Performance tests](test/performance/README.md) section of the repository includes information to run performance
tests on the STH component. If you are interested on them, please navigate to that section of the repository for further information.

[Top](#section0)

##<a id="section9"></a>Additional resources
The [Additional resources](resource/README.md) section of the repository includes some scripts and utilities which may make the developer's life easier.
If you are interested on them, please navigate to that section of the repository for further information.

[Top](#section0)

##<a id="section10"></a>Contribution guidelines

### <a id="section10.1"></a> Overview
Being an open source project, everyone can contribute, provided that it respects the following points:
* Before contributing any code, the author must make sure all the tests work (see below how to run the tests).
* Developed code must adhere to the syntax guidelines enforced by the linters.
* Code must be developed following the branching model and changelog policies defined below.
* For any new feature added, unit tests must be provided, following the example of the ones already created.

In order to start contributing:
1. Fork this repository clicking on the "Fork" button on the upper-right area of the page.
2. Clone your just forked repository:
```
git clone https://github.com/your-github-username/fiware-sth-comet.git
```
3. Add the main fiware-sth-comet repository as a remote to your forked repository (use any name for your remote
repository, it does not have to be fiware-sth-comet, although we will use it in the next steps):
```
git remote add fiware-sth-comet https://github.com/telefonicaid/fiware-sth-comet.git
```

Before starting your contribution, remember to synchronize the `develop` branch in your forked repository with the `develop`
branch in the main lfiware-sth-comet repository following the next steps:

1. Change to your local `develop` branch (in case you are not in it already):
```
  git checkout develop
```
2. Fetch the remote changes:
```
  git fetch fiware-sth-comet
```
3. Merge them:
```
  git rebase fiware-sth-comet/develop
```

Contributions following these guidelines will be added to the `develop` branch, and released in the next version. The
release process is explained in the [Releasing](#section10.9) section below.


###<a id="section10.2"></a> Branching model
There are two special branches in the repository:

* `master`: holds the code for the last stable version of the project. It is only updated when a new version is released.
* `develop`: contains the last stable development code. New features and bug fixes are always merged to `develop`.

In order to start developing a new feature or refactoring, a new branch should be created with name `task/<taskName>`
in the newly forked repository.
This new branch must be created from the current version of the `develop` branch (remember to fetch the latest changes from
the remote `develop` branch before creating this new branch).
Once the new functionality has been completed, a pull request should be created from the
new branch to the `develop` branch in the main remote repository.
Remember to check both the linters and the tests before creating the pull request.

Fixing bugs follow the same branching guidelines as in the case of adding a new feature or refactoring code with the
exception of the branch name. In the case of bug fixes, the new branch should be called `bug/<bugName>`.

There are another set of branches called `release/<versionNumber>`, one for each version of the product. These branches
point to each one of the released versions of the project. They are permanent and they are created with each release.

###<a id="section10.3"></a> Changelog
The project contains a version changelog file, called `CHANGES_NEXT_RELEASE`, that can be found in the root of the project.
Whenever a new feature or bug fix is going to be merged with `develop`, a new entry should be added to this changelog.
The new entry should contain the reference number of the issue it is solving (if any).

When a new version is released, the changelog is cleared, and remains fixed in the last commit of that version. The
content of the changelog is also moved to the release description in the Github release.

###<a id="section10.4"> Coding guidelines
Coding guidelines are defined via the provided `.jshintrc` and `.gjslintrc` flag files. The latter requires Python and
its use can be disabled while creating the project skeleton with grunt-init.
To check source code style, type:
```bash
grunt lint
```

Checkstyle reports can be used together with Jenkins to monitor project quality metrics by means of Checkstyle
and Violations plugins.
To generate Checkstyle and JSLint reports under `report/lint/`, type:
```bash
grunt lint-report
```

###<a id="section10.5"></a> Testing
The test environment is preconfigured to run the [Mocha](http://visionmedia.github.io/mocha/) Test Runner with support
for the [Chai](http://chaijs.com/) assertion library as well as for [Sinon](http://sinonjs.org/) spies, stubs, etc.,
following a [BDD](http://chaijs.com/api/bdd/) testing style with `chai.expect` and `chai.should()` available globally
while executing tests, as well as the [Sinon-Chai](http://chaijs.com/plugins/sinon-chai) plugin.

Module mocking during testing can be done with [proxyquire](https://github.com/thlorenz/proxyquire).

To run tests, type:
```bash
grunt test
```

Tests reports can be used together with Jenkins to monitor project quality metrics by means of TAP or XUnit plugins.
To generate TAP report in `report/test/unit_tests.tap`, type
```bash
grunt test-report
```

###<a id="section10.6"></a> Continuous testing
Support for continuous testing is provided so that tests are run when any source file or test is modified.
For continuous testing, type:
```bash
grunt watch
```

###<a id="section10.7"></a> Source code documentation
HTML code documentation can be generated under the `site/doc/` path. It can be used together with Jenkins by means of
DocLinks plugin.
For compiling source code documentation, type:
```bash
grunt doc
```

###<a id="section10.8"></a> Code coverage
A very good practice is to measure the code coverage of your tests.

To generate an HTML coverage report under the `site/coverage/` path and to print out a summary, type:
```bash
# Use git-bash on Windows
grunt coverage
```

To generate a Cobertura report in `report/coverage/cobertura-coverage.xml` that can be used together with Jenkins to
monitor project quality metrics by means of Cobertura plugin, type
```bash
# Use git-bash on Windows
grunt coverage-report
```

###<a id="section10.8"></a> Code complexity
Another very good practice is to analise code complexity.

Support for using Plato and storing the generated report in the `site/report/` path is provided. This capability can be
used together with Jenkins by means of DocLinks plugin.

To generate a code complexity report, type:
```bash
grunt complexity
```

###<a id="section10.9"></a> Releasing
The process of making a release consists of the following steps and should be made by any of the owners or administrators
of the main repository:

1. Create a new task branch changing the development version number in the `package.json` file (with a suffix `-next`)
to the new target version (without any suffix), and create a pull request of this new task branch into `develop`. Remember
to delete the temporary created task branch.
2. Create a release branch named `release/<versionNumber>` from the last version of `develop` using the corresponding
version number.
3. Create a new release in Github setting the tag version as `release-<versionNumber>` from the new release branch
`release/<versionNumber>` and publish it.
5. Create a pull request from the new release branch `release/<versionNumber>` to `master`.
6. Create a new task branch to prepare the `develop` branch for the next release, adding the `-next` suffix to the
current version number in the `package.json` file (to signal this as the development version) and removing the contents
of the `CHANGES_NEXT_RELEASE` changelog file. Create a pull request from this new task branch to `develop`.
Remember to delete the temporary created task branch.

To further guide you through your first contributions, we have created the label [```mentored```](https://github.com/telefonicaid/fiware-sth-comet/labels/mentored)
which are assigned to those bugs and issues simple and interesting enough to be solved by people new to the project.
Feel free to assign any of them to yourself and do not hesitate to mention any of the main developers
(this is, [@gtorodelvalle](https://github.com/gtorodelvalle) or [@frbattid](https://github.com/frbattid))
in the issue's comments to get help from them during its resolution. They will be glad to help you.

[Top](#section0)

##<a id="section11"></a>Contact
* Germán Toro del Valle (<a href="mailto:german.torodelvalle@telefonica.com">german.torodelvalle@telefonica.com</a>, <a href="http://www.twitter.com/gtorodelvalle" target="_blank">@gtorodelvalle</a>)
* Francisco Romero Bueno (<a href="mailto:francisco.romerobueno@telefonica.com">francisco.romerobueno@telefonica.com</a>)
* Iván Arias León (<a href="mailto:ivan.ariasleon@telefonica.com">ivan.ariasleon@telefonica.com</a>)

[Top](#section0)
